---
sidebar_position: 4
sidebar_label: تقلیدی تعلیم اور LfD
---

# ہیومنائیڈز کے لیے تقلیدی تعلیم اور مظاہرے سے سیکھنا

## خلاصہ

*   **سبق 1 - نگرانی شدہ تعلیم:** ادراک کے لیے لیبل شدہ ڈیٹا کے ساتھ ماڈلز کی تربیت۔
*   **سبق 2 - غیر نگرانی شدہ تعلیم:** غیر لیبل شدہ ڈیٹا میں پوشیدہ نمونے دریافت کرنا۔
*   **سبق 3 - تقویتی تعلیم:** انعامات اور سزاؤں کے ذریعے بہترین طرز عمل سیکھنا۔

جبکہ تقویتی تعلیم ہیومنائیڈز کو نئے طرز عمل دریافت کرنے کی اجازت دیتی ہے، یہ خاص طور پر پیچیدہ، کثیر مرحلہ کاموں کے لیے انتہائی سست، نمونے کے لحاظ سے غیر موثر، اور ممکنہ طور پر تلاش کے دوران غیر محفوظ ہو سکتی ہے۔ **تقلیدی تعلیم (IL)** اور **مظاہرے سے سیکھنا (LfD)** ایک زیادہ بدیہی اور اکثر تیز متبادل پیش کرتے ہیں: ہیومنائیڈز کو ایک انسان (یا ماہر روبوٹ) کو کام انجام دیتے ہوئے مشاہدہ کرکے سیکھنے کی اجازت دینا۔ یہ محتاط انعامی فنکشن ڈیزائن اور وسیع تلاش کی ضرورت کو نظرانداز کرتا ہے، جس سے یہ ہیومنائیڈز کو نئی مہارتیں سکھانے کے لیے انتہائی پرکشش بن جاتا ہے۔

### 1. تقلیدی تعلیم (IL) کی بنیادی باتیں

تقلیدی تعلیم مسئلے کو ایک نگرانی شدہ تعلیم کے کام کے طور پر فریم کرتی ہے۔ روبوٹ کو (state, action) جوڑوں کا ایک ڈیٹاسیٹ دیا جاتا ہے، جہاں اعمال متعلقہ حالتوں میں ایک ماہر کے ذریعے انجام دیے جاتے ہیں۔ مقصد ایک پالیسی سیکھنا ہے جو حالتوں کو اعمال سے منسلک کرتی ہے، ماہر کے طرز عمل کی نقل کرتے ہوئے۔

#### کلیدی طریقے:

*   **طرز عملی کلوننگ (BC):** IL کی سادہ ترین شکل۔ ماہر رفتار کے ڈیٹاسیٹ کو دیکھتے ہوئے، BC ایک پالیسی (اکثر ایک نیورل نیٹ ورک) کو تربیت دیتا ہے تاکہ مشاہدہ شدہ حالتوں کو براہ راست ماہر کے اعمال سے منسلک کیا جا سکے۔ یہ بنیادی طور پر ایک نگرانی شدہ تعلیم کا مسئلہ ہے۔
    *   **فوائد:** نافذ کرنا آسان، تربیت میں تیز۔
    *   **نقصانات:** مجموعی خرابیوں (تقسیم کی منتقلی) کا شکار۔ اگر روبوٹ ماہر کی رفتار سے تھوڑا سا ہٹ جائے، تو یہ ایسی حالتوں کا سامنا کرتا ہے جو اس نے پہلے نہیں دیکھیں، اور اصلاحی تاثرات کے بغیر، یہ مطلوبہ راستے سے مزید اور مزید دور ہو سکتا ہے۔
*   **ڈیٹاسیٹ جمع (DAgger):** BC کے مجموعی خرابی کے مسئلے کا حل پیش کرتا ہے۔ روبوٹ پہلے BC کے ساتھ ایک پالیسی کی تربیت کرتا ہے۔ پھر، یہ ماحول میں پالیسی پر عمل کرتا ہے، اور اگر یہ ایسی حالتوں کا سامنا کرتا ہے جن کے بارے میں یہ غیر یقینی ہے، تو یہ اس حالت میں صحیح عمل کے لیے ماہر سے سوال کرتا ہے۔ ان نئے (state, action) جوڑوں کو ڈیٹاسیٹ میں شامل کیا جاتا ہے، اور پالیسی کو دوبارہ تربیت دی جاتی ہے۔ یہ عمل تکراری طور پر دہرایا جاتا ہے۔
    *   **فوائد:** مجموعی خرابیوں کو کم کرتا ہے، مضبوطی کو بہتر بناتا ہے۔
    *   **نقصانات:** ایک "آن لائن" ماہر کی ضرورت ہوتی ہے جو روبوٹ کی عملداری کے دوران تاثرات فراہم کر سکے۔

### 2. مظاہرے سے سیکھنا (LfD) کی تکنیکیں

LfD ایک وسیع تر اصطلاح ہے جو مختلف طریقوں پر محیط ہے جہاں ایک روبوٹ مظاہروں کا مشاہدہ کرکے سیکھتا ہے۔ اس میں اکثر صرف براہ راست state-action میپنگ سے زیادہ شامل ہوتا ہے۔

#### عام LfD طریقے:

*   **Kinesthetic ٹیچنگ (پروگرامنگ بذریعہ مظاہرہ):** انسان جسمانی طور پر روبوٹ کے end-effector یا جوڑوں کی پوزیشنوں کو مطلوبہ حرکت کے ذریعے رہنمائی کرتا ہے۔ روبوٹ جوڑوں کی رفتار کو ریکارڈ کرتا ہے اور انہیں دوبارہ پیدا کرنا سیکھتا ہے۔ یہ صنعتی روبوٹس کے لیے عام ہے۔
    *   **فوائد:** براہ راست، سادہ رفتار کے لیے بدیہی۔
    *   **نقصانات:** پیچیدہ، کثیر رابطے والے کاموں کے لیے یا مختلف روبوٹ شکلوں میں مہارتوں کی منتقلی کے لیے مشکل ہو سکتا ہے۔
*   **Teleoperation:** ایک انسان روبوٹ کو دور سے جوائے اسٹک، ماسٹر ڈیوائس (جیسے haptic بازو)، یا یہاں تک کہ VR انٹرفیس استعمال کرتے ہوئے چلاتا ہے۔ روبوٹ انسان کے احکامات اور اس کے نتیجے میں حرکات کو ریکارڈ کرتا ہے۔
*   **ہدف سے مشروط LfD:** صحیح رفتار سیکھنے کے بجائے، روبوٹ مظاہرے سے *کیا* ہدف کی حالت ہے سیکھتا ہے اور پھر اس ہدف تک پہنچنے کے لیے اپنی خود کی منصوبہ بندی یا RL صلاحیتوں کا استعمال کرتا ہے۔
*   **الٹا تقویتی تعلیم (IRL):** براہ راست پالیسی سیکھنے کے بجائے، IRL ماہر کے **انعامی فنکشن** کو ان کے مظاہروں سے استنباط کرنے کی کوشش کرتا ہے۔ ایک بار جب انعامی فنکشن سیکھا جاتا ہے، تو روبوٹ پھر RL (یا دیگر بہتری کے طریقے) استعمال کر سکتا ہے تاکہ ایک پالیسی تلاش کرے جو اس استنباط شدہ انعام کو زیادہ سے زیادہ کرے، ممکنہ طور پر براہ راست تقلید سے زیادہ مضبوط اور عمومی طرز عمل کی طرف لے جا سکتا ہے۔
    *   **فوائد:** بنیادی مقصد کو پکڑ کر زیادہ مضبوط پالیسیوں کی طرف لے جا سکتا ہے۔
    *   **نقصانات:** کمپیوٹیشنل طور پر شدید، انعامی فنکشن کا استنباط مبہم ہو سکتا ہے۔

**کوڈ سنپٹ مثال (تصوراتی طرز عملی کلوننگ):**

```python
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from sklearn.model_selection import train_test_split

# --- 1. Simulate Expert Demonstrations ---
# Imagine an expert human controlling a simple humanoid's arms (2 joints)
# State: current joint angles (theta1, theta2) - 2 features
# Action: desired torque for each joint (torque1, torque2) - 2 features

num_demonstrations = 1000
expert_states = np.random.uniform(low=-np.pi, high=np.pi, size=(num_demonstrations, 2))
expert_actions = np.zeros((num_demonstrations, 2))

# Create a simple "expert policy" rule:
# If theta1 > 0, torque1 is positive; if theta1 < 0, torque1 is negative.
# If theta2 is around pi/2, torque2 is zero; otherwise push towards pi/2.
expert_actions[:, 0] = expert_states[:, 0] * 0.5 + np.random.normal(0, 0.1, num_demonstrations) # proportional control + noise
expert_actions[:, 1] = (expert_states[:, 1] - np.pi/2) * -0.3 + np.random.normal(0, 0.1, num_demonstrations) # to reach pi/2 + noise

# Split data
X_train, X_test, y_train, y_test = train_test_split(expert_states, expert_actions, test_size=0.2, random_state=42)

# --- 2. Build a Behavioral Cloning Model (Neural Network) ---
model = Sequential([
    Dense(64, activation='relu', input_shape=(2,)), # Input: 2 joint angles
    Dense(64, activation='relu'),
    Dense(2, activation='linear') # Output: 2 torques (continuous)
])

# --- 3. Compile the Model ---
model.compile(optimizer='adam', loss='mse') # Mean Squared Error for regression

# --- 4. Train the Model (conceptual) ---
# history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test), verbose=0)
# print("Behavioral Cloning model trained conceptually.")
# print(f"Final training loss: {history.history['loss'][-1]:.4f}")
# print(f"Final validation loss: {history.history['val_loss'][-1]:.4f}")

# --- 5. Use the Trained Policy (conceptual) ---
# simulated_state = np.array([[0.5, 2.0]]) # Example new state
# predicted_action = model.predict(simulated_state)
# print(f"Simulated state: {simulated_state}")
# print(f"Predicted action (torques): {predicted_action}")

```

### ہیومنائیڈز کے لیے تقلیدی تعلیم میں چیلنجز

*   **مجموعی خرابیاں (تقسیم کی منتقلی):** جیسے ہی روبوٹ سیکھی ہوئی پالیسی پر عمل کرتا ہے، یہ ایسی حالتوں کا سامنا کر سکتا ہے جو ماہر کے مظاہروں سے تھوڑی مختلف ہیں۔ اصلاحی تاثرات کے بغیر، یہ چھوٹی خرابیاں جمع ہو سکتی ہیں، جو اہم انحرافات کی طرف لے جاتی ہیں۔ DAgger اسے کم کرنے میں مدد کرتا ہے۔
*   **ذیلی بہتری:** روبوٹ صرف اپنے استاد کی طرح اچھا ہو سکتا ہے۔ اگر ماہر غلطیاں کرتا ہے، تو روبوٹ انہیں سیکھے گا۔
*   **عمومیت:** سیکھی ہوئی مہارتوں کو نئے ماحول یا کاموں میں منتقل کرنا جو مظاہرے کے ڈیٹا سے کافی مختلف ہیں مشکل ہو سکتا ہے۔
*   **تجسیم کی عدم مطابقت:** انسانی مظاہروں سے سیکھنا اور مختلف kinematics، dynamics، یا sensing صلاحیتوں والے روبوٹ پر لاگو کرنا چیلنجنگ ہو سکتا ہے۔
*   **"ماہر" کی تعریف:** پیچیدہ مہارتوں کے لیے، مسلسل اعلیٰ معیار کے مظاہرے حاصل کرنا مشکل ہو سکتا ہے۔

### سرگرمیاں

1.  **انسانی مہارت کا مشاہدہ:** ایک سادہ انسانی مہارت منتخب کریں (جیسے، گلاس میں پانی ڈالنا، دروازہ کھولنا)۔ کسی کو اسے انجام دیتے ہوئے دیکھیں۔ کلیدی حالتوں (جیسے، "ہاتھ گلاس کے قریب"، "پانی کی سطح بڑھ رہی ہے") اور اعمال (جیسے، "بازو نیچے لے جائیں"، "کلائی گھمائیں") کی فہرست بنائیں۔ آپ طرز عملی کلوننگ استعمال کرتے ہوئے روبوٹ کے لیے اسے کیسے ریکارڈ کریں گے؟
2.  **مجموعی خرابیوں کا حل:** DAgger الگورتھم کی مزید تفصیل سے تحقیق کریں۔ وضاحت کریں کہ یہ خاص طور پر طرز عملی کلوننگ میں مجموعی خرابیوں کے مسئلے کا کیسے حل پیش کرتا ہے۔ اس کی عملی حدود کیا ہیں؟

### ڈایاگرام

_طرز عملی کلوننگ کے عمل کو ظاہر کرنے والے ڈایاگرام کے لیے placeholder: Human Expert -> (State, Action) data -> Supervised Learning -> Robot Policy۔ تقسیم کی منتقلی کے تصور کو نمایاں کریں۔_
*(یہ تصویر `/static/img/diagrams/part3-ch3-lesson4-imitation-learning.svg` میں محفوظ کی جائے گی)*

### کثیر الانتخابی سوالات

1.  **تقلیدی تعلیم (IL)** کے پیچھے بنیادی خیال کیا ہے؟
    a) انعامات کے ساتھ آزمائش اور خطا کے ذریعے بہترین طرز عمل سیکھنا۔
    b) ایک ماہر کو کام انجام دیتے ہوئے مشاہدہ کرکے سیکھنا۔
    c) غیر لیبل شدہ ڈیٹا میں پوشیدہ نمونے دریافت کرنا۔
    d) ہر روبوٹ کی حرکت کو براہ راست پروگرام کرنا۔
    **جواب: b**

2.  **طرز عملی کلوننگ** کا بنیادی چیلنج کون سا ہے؟
    a) اس میں انعامی فنکشنز کی تعریف کے لیے ماہر کی ضرورت ہوتی ہے۔
    b) یہ تربیت میں بہت سست ہے۔
    c) یہ مجموعی خرابیوں (تقسیم کی منتقلی) کا شکار ہے۔
    d) اسے ہیومنائیڈ روبوٹس پر لاگو نہیں کیا جا سکتا۔
    **جواب: c**

3.  **ڈیٹاسیٹ جمع (DAgger)** طرز عملی کلوننگ کو اس طرح بہتر بناتا ہے:
    a) کسی بھی ماہر مظاہرے کی ضرورت کو ختم کرنا۔
    b) صرف غیر نگرانی شدہ تعلیم کی ضرورت۔
    c) تکراری طور پر ان حالتوں کے لیے نئے ماہر لیبلز جمع کرنا جن کا روبوٹ عملداری کے دوران سامنا کرتا ہے۔
    d) ماہر کے انعامی فنکشن کا استنباط کرنے پر توجہ مرکوز کرنا۔
    **جواب: c**

4.  **Kinesthetic ٹیچنگ** LfD کی ایک شکل ہے جہاں:
    a) انسان جوائے اسٹک استعمال کرتے ہوئے روبوٹ کو دور سے کنٹرول کرتا ہے۔
    b) انسان جسمانی طور پر روبوٹ کی حرکات کو مطلوبہ حرکت کے ذریعے رہنمائی کرتا ہے۔
    c) روبوٹ انسان کے انعامی فنکشن کا استنباط کرتا ہے۔
    d) روبوٹ ہدایات پڑھ کر کام انجام دینا سیکھتا ہے۔
    **جواب: b**

5.  **الٹا تقویتی تعلیم (IRL)** ماہر مظاہروں سے کس چیز کا استنباط کرنے کا مقصد رکھتا ہے؟
    a) ماہر کی صحیح جوڑوں کی رفتار۔
    b) ماہر کی سینسر ریڈنگز۔
    c) ماہر کا بنیادی انعامی فنکشن۔
    d) ماہر کی ترجیحی پروگرامنگ زبان۔
    **جواب: c**

6.  اگر طرز عملی کلوننگ کے ساتھ تربیت یافتہ ایک ہیومنائیڈ روبوٹ ایک ایسی حالت کا سامنا کرتا ہے جو اس نے تربیتی ڈیٹا میں نہیں دیکھی اور غلط عمل انجام دیتا ہے، تو یہ اس کی مثال ہے:
    a) انعام کی کمی۔
    b) پالیسی کی بہتری۔
    c) تقسیم کی منتقلی (مجموعی خرابیاں)۔
    d) عمومیت کی کامیابی۔
    **جواب: c**

7.  پیچیدہ کاموں کے لیے تقویتی تعلیم کے مقابلے میں **تقلیدی تعلیم** کا ایک بڑا فائدہ ہے:
    a) یہ عالمی سطح پر بہترین پالیسی کی ضمانت دیتا ہے۔
    b) یہ وسیع تلاش اور انعامی فنکشن ڈیزائن کی ضرورت سے بچتا ہے۔
    c) یہ ہمیشہ ذیلی بہترین مظاہروں سے سیکھ سکتا ہے۔
    d) یہ تربیت کے دوران جسمانی روبوٹس کے لیے فطری طور پر محفوظ ہے۔
    **جواب: b**

8.  **Teleoperation** LfD کا ایک طریقہ ہے جو شامل ہے:
    a) روبوٹ انسانوں کی ویڈیوز دیکھ کر مہارتیں سیکھتا ہے۔
    b) ایک انسان مظاہرہ ڈیٹا تیار کرنے کے لیے روبوٹ کو دور سے چلاتا ہے۔
    c) روبوٹ انسانی حرکات کو نقل کرنے کے لیے اپنے اندرونی سینسرز استعمال کرتا ہے۔
    d) روبوٹس ایک دوسرے کے ساتھ telepathically بات چیت کرتے ہیں۔
    **جواب: b**

9.  ہیومنائیڈز میں تقلیدی تعلیم کے لیے **عمومیت** کیوں ایک چیلنج ہے؟
    a) کیونکہ سیکھی ہوئی مہارتیں اکثر مظاہروں سے مختلف ماحول یا کاموں میں خراب کارکردگی دکھاتی ہیں۔
    b) کیونکہ اس میں بہت زیادہ کمپیوٹیشنل طاقت کی ضرورت ہوتی ہے۔
    c) کیونکہ انسانی ماہر ہمیشہ کامل ہوتا ہے۔
    d) کیونکہ روبوٹ انسانی زبان کو نہیں سمجھ سکتا۔
    **جواب: a**

10. LfD میں **تجسیم کی عدم مطابقت** کا چیلنج کیا ہے؟
    a) جب روبوٹ کا سافٹ ویئر اس کے ہارڈ ویئر کے ساتھ غیر مطابق ہو۔
    b) جب انسانی مظاہروں سے سیکھنا اور مختلف جسمانی خصوصیات والے روبوٹ پر لاگو کرنا۔
    c) جب روبوٹ انٹرنیٹ سے منسلک نہیں ہو سکتا۔
    d) جب روبوٹ کے پاس بہت زیادہ سینسرز ہیں۔
    **جواب: b**
