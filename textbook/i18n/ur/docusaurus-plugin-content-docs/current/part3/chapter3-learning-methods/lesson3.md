---
sidebar_position: 3
sidebar_label: مہارت حصول کے لیے تقویتی تعلیم
---

# ہیومنائیڈ مہارت حصول اور کنٹرول کے لیے تقویتی تعلیم

## خلاصہ

*   **سبق 1 - نگرانی شدہ تعلیم:** ادراک اور پیشن گوئی کے لیے لیبل شدہ ڈیٹا سے سیکھنا۔
*   **سبق 2 - غیر نگرانی شدہ تعلیم:** بے ضابطگی کا پتہ لگانے اور نمائندگی کے لیے غیر لیبل شدہ ڈیٹا میں نمونے دریافت کرنا۔

جیسے جیسے ہیومنائیڈز بنیادی ادراک سے آگے بڑھتے ہیں اور خودمختار عمل میں داخل ہوتے ہیں، پیچیدہ مہارتیں حاصل کرنے کی صلاحیت انتہائی اہم ہو جاتی ہے۔ **تقویتی تعلیم (RL)**، جس کا ہم نے حصہ 3، باب 2، سبق 3 میں فیصلہ سازی کے لیے ذکر کیا تھا، ہیومنائیڈز میں **مہارت حصول اور نچلی سطح کے کنٹرول** کے لیے بھی ایک طاقتور نمونہ ہے۔ ہر حرکت کو واضح طور پر پروگرام کرنے کے بجائے، RL روبوٹس کو موٹر پالیسیاں سیکھنے کی اجازت دیتا ہے جو آزمائش اور خطا کے ذریعے مطلوبہ نتائج حاصل کرتی ہیں، کارکردگی، مضبوطی، اور موافقت کو بہتر بناتی ہیں۔

### 1. موٹر کنٹرول اور حرکت کے لیے RL

متحرک موٹر کنٹرول پالیسیاں سیکھنا ہیومنائیڈ روبوٹکس میں RL کی سب سے کامیاب ایپلیکیشنز میں سے ایک ہے:

*   **حرکت:** ہیومنائیڈز نے RL استعمال کرتے ہوئے چلنا، دوڑنا، چھلانگ لگانا، اور سیڑھیاں چڑھنا سیکھا ہے۔ انعامی فنکشن کو استحکام، رفتار، اور توانائی کی کارکردگی کی حوصلہ افزائی کے لیے ڈیزائن کیا جا سکتا ہے۔ مثال کے طور پر، بغیر گرے آگے بڑھنے کے لیے مثبت انعام، اور گرنے کے لیے بڑا منفی انعام۔
*   **توازن:** توازن برقرار رکھنا ایک مسلسل کنٹرول مسئلہ ہے جس میں RL بہترین ہے۔ پالیسیاں جوڑوں کے ٹارک کو ایڈجسٹ کرکے بیرونی خلل (دھکے، ناہموار زمین) پر رد عمل دینا سیکھ سکتی ہیں۔
*   **ہیرا پھیری کی مہارتیں:** مختلف شکلوں اور وزن والی اشیاء کو پکڑنا، دروازے کھولنا، یا آلات استعمال کرنا سیکھنا۔ انعامی سگنل کامیاب کام مکمل ہونے سے آ سکتے ہیں (جیسے، چیز محفوظ طریقے سے ہاتھ میں)۔

### 2. ہیومنائیڈز کے لیے گہری تقویتی تعلیم (DRL)

گہرے نیورل نیٹ ورکس ہیومنائیڈز کی خصوصیت والے اعلیٰ جہتی مسلسل حالت اور عمل کی جگہوں میں RL کے لیے درکار فنکشن تخمینہ کی صلاحیتیں فراہم کرتے ہیں۔

*   **پالیسی نیٹ ورکس:** نیورل نیٹ ورکس براہ راست حالتوں (جیسے، جوڑوں کے زاویے، سینسر ریڈنگز) کو اعمال (جیسے، جوڑوں کے ٹارک یا ہدف زاویے) سے منسلک کرتے ہیں۔
*   **ویلیو نیٹ ورکس:** نیورل نیٹ ورکس دی گئی حالت سے متوقع مستقبل کے انعام کا تخمینہ لگاتے ہیں، پالیسی اپ ڈیٹس کی رہنمائی میں مدد کرتے ہیں۔
*   **مسلسل کنٹرول الگورتھمز:** الگورتھمز جیسے Proximal Policy Optimization (PPO)، Soft Actor-Critic (SAC)، اور Twin-Delayed DDPG (TD3) ہیومنائیڈز میں مسلسل کنٹرول کاموں کے لیے اکثر استعمال ہوتے ہیں۔ یہ جوڑوں کی پوزیشنوں، رفتاروں، اور ٹارک کی مسلسل نوعیت کو سنبھالتے ہیں۔

### 3. Sim-to-Real منتقلی اور ڈومین بے ترتیبی

روبوٹکس میں RL کے لیے ایک بڑی رکاوٹ **sim-to-real gap** ہے – سمیولیشن میں سیکھی گئی پالیسیاں اکثر جسمانی روبوٹ میں منتقل ہونے پر خراب کارکردگی دکھاتی ہیں۔ اس خلا کو پر کرنے کی حکمت عملیوں میں شامل ہیں:

*   **ڈومین بے ترتیبی:** تربیت کے دوران سمیولیشن میں مختلف پیرامیٹرز (جیسے، رگڑ کے گتانک، جوڑوں کے ماس، سینسر شور، تاخیر) کو بے ترتیب کرنا۔ یہ RL پالیسی کو تغیرات کے لیے مضبوط بننے پر مجبور کرتا ہے، اسے حقیقی دنیا میں عمومیت کے لیے زیادہ امکان فراہم کرتا ہے۔
*   **سسٹم شناخت:** حقیقی روبوٹ سے ڈیٹا استعمال کرتے ہوئے زیادہ درست سمیولیشن ماڈل بنانا۔
*   **حقیقی دنیا میں بہتری:** حقیقی دنیا کی تربیتی ڈیٹا کی تھوڑی مقدار سمیولیشن میں پہلے سے تربیت یافتہ پالیسی کو بہتر بنانے کے لیے استعمال کی جا سکتی ہے۔

### 4. درجہ بندی تقویتی تعلیم (HRL)

پیچیدہ ہیومنائیڈ طرز عمل کو ذیلی مہارتوں میں تقسیم کیا جا سکتا ہے۔ HRL متعدد سطحوں پر سیکھنے کے عمل کو منظم کرکے اس کا حل پیش کرتا ہے:

*   **اعلیٰ سطح کی پالیسی:** ذیلی مہارتوں کی ترتیب منتخب کرنا سیکھتی ہے (جیسے، "چلنا"، "پہنچنا"، "پکڑنا")۔
*   **نچلی سطح کی پالیسیاں:** ہر ذیلی مہارت ایک الگ RL ایجنٹ کے ذریعے سیکھی جاتی ہے، اس کے مخصوص مقصد کے لیے بہتر بناتے ہوئے۔
یہ مجموعی مسئلے کی پیچیدگی کو کم کرتا ہے اور سیکھنے کی کارکردگی کو بہتر بناتا ہے۔

**کوڈ سنپٹ مثال (حرکت کے لیے تصوراتی انعامی فنکشن):**

```python
# Conceptual Python: Simplified Reward Function for Humanoid Walking
class WalkingRewardFunction:
    def __init__(self, target_forward_vel=0.5, desired_height=0.8, com_stability_threshold=0.1):
        self.target_forward_vel = target_forward_vel
        self.desired_height = desired_height
        self.com_stability_threshold = com_stability_threshold

    def calculate_reward(self, current_state, next_state, action):
        reward = 0.0

        # 1. Encourage forward movement (maximize forward velocity)
        forward_velocity_reward = (next_state.forward_velocity - self.target_forward_vel)**2 # Negative quadratic
        reward -= 5.0 * forward_velocity_reward # Penalize deviation from target velocity

        # 2. Penalize falling (stability)
        if next_state.is_falling:
            reward -= 1000.0 # Large penalty for falling

        # 3. Encourage maintaining desired height
        height_error = abs(next_state.com_height - self.desired_height)
        reward -= 2.0 * height_error # Penalize deviation from desired height

        # 4. Penalize excessive joint torques/energy consumption (smoothness/efficiency)
        # Assuming next_state has sum_of_abs_torques
        reward -= 0.1 * next_state.sum_of_abs_torques

        # 5. Reward for CoM stability (keep CoM projection within support polygon)
        # Conceptual: Assuming next_state.com_within_support_polygon is a boolean
        if next_state.com_within_support_polygon:
             reward += 10.0
        else:
             reward -= 50.0 # Moderate penalty if CoM leaves support polygon

        # A small positive reward for just staying alive/making progress
        reward += 1.0

        return reward

# Example Usage (conceptual states)
class RobotState:
    def __init__(self, forward_vel, is_falling, com_height, com_within_sp, sum_torques):
        self.forward_velocity = forward_vel
        self.is_falling = is_falling
        self.com_height = com_height
        self.com_within_support_polygon = com_within_sp
        self.sum_of_abs_torques = sum_torques

reward_func = WalkingRewardFunction()

# State after a good step
good_state = RobotState(0.48, False, 0.79, True, 50.0)
# State after a stumble
stumble_state = RobotState(0.2, False, 0.70, False, 120.0)
# State after a fall
fall_state = RobotState(0.0, True, 0.3, False, 200.0)

print(f"Reward for good step: {reward_func.calculate_reward(None, good_state, None):.2f}")
print(f"Reward for stumble: {reward_func.calculate_reward(None, stumble_state, None):.2f}")
print(f"Reward for fall: {reward_func.calculate_reward(None, fall_state, None):.2f}")

```

### ہیومنائیڈ مہارتوں کے لیے RL میں چیلنجز

*   **کم انعامات:** بہت سے حقیقی دنیا کے ہیومنائیڈ کام صرف آخر میں انعام فراہم کرتے ہیں (جیسے، "کامیابی سے چیز پکڑی گئی")، جس سے روبوٹ کے لیے یہ سیکھنا مشکل ہو جاتا ہے کہ کن درمیانی اعمال نے کامیابی میں حصہ ڈالا۔
*   **ایکسپلوریشن-استحصال کا مخمصہ:** بہتر حکمت عملیاں دریافت کرنے کے لیے نئے اعمال تلاش کرنے کی ضرورت بمقابلہ معلوم اچھی حکمت عملیوں کا استعمال کرنے کے درمیان توازن۔
*   **ایکسپلوریشن کی حفاظت:** بے ترتیب ایکسپلوریشن غیر محفوظ حرکات یا روبوٹ یا ماحول کو نقصان کا سبب بن سکتا ہے۔
*   **مؤثر انعامی فنکشنز ڈیزائن کرنا:** ایسے انعامی فنکشنز ہاتھ سے تیار کرنا جو غیر ارادی ضمنی اثرات کے بغیر مطلوبہ پیچیدہ طرز عمل کی طرف لے جائیں ایک فن اور سائنس ہے۔
*   **کمپیوٹیشنل لاگت:** پیچیدہ DRL پالیسیاں تربیت دینے کے لیے بہت زیادہ کمپیوٹیشنل وسائل کی ضرورت ہوتی ہے، اکثر بڑے GPU کلسٹرز اور وسیع سمیولیشن وقت شامل ہوتا ہے۔

### سرگرمیاں

1.  **انعام کی شکل دہی:** ایک ہیومنائیڈ پر غور کریں جو دروازہ کھولنا سیکھ رہا ہے۔ انعام کی شکل دہی استعمال کرتے ہوئے ایک انعامی فنکشن ڈیزائن کریں – مرکزی مقصد کو چھوٹے ذیلی اہداف میں توڑنا (جیسے، دروازے کے قریب جانا، ہینڈل پکڑنا، ہینڈل گھمانا، دروازہ دھکیلنا/کھینچنا)۔ یہ روبوٹ کو ایک واحد کام کے اختتام کے انعام سے کیسے تیزی سے سیکھنے میں مدد کرے گا؟
2.  **ڈومین بے ترتیبی کا خیال:** حقیقی دنیا میں مختلف زمینوں پر مضبوط چلنے کے لیے ایک ہیومنائیڈ کو تربیت دینے کے لیے سمیولیشن میں تین مختلف جسمانی پیرامیٹرز پر غور کریں جو آپ بے ترتیب کریں گے۔ اپنے انتخاب کا جواز دیں۔

### ڈایاگرام

_Sim-to-Real منتقلی کے عمل کو ظاہر کرنے والے ڈایاگرام کے لیے placeholder، جو بے ترتیب سمیولیشن ماحول میں تربیت یافتہ پالیسی کو جسمانی ہیومنائیڈ روبوٹ میں تعینات کرتے ہوئے دکھاتا ہے۔_
*(یہ تصویر `/static/img/diagrams/part3-ch3-lesson3-rl-skills.svg` میں محفوظ کی جائے گی)*

### کثیر الانتخابی سوالات

1.  ہیومنائیڈ مہارت حصول کے لیے **تقویتی تعلیم** استعمال کرنے کا بنیادی فائدہ کیا ہے؟
    a) اس میں ہر ممکنہ حرکت کے لیے واضح پروگرامنگ کی ضرورت ہوتی ہے۔
    b) یہ روبوٹس کو آزمائش اور خطا کے ذریعے موٹر پالیسیاں سیکھنے کی اجازت دیتا ہے، مطلوبہ نتائج کے لیے بہتر بناتے ہوئے۔
    c) یہ کم سے کم تربیتی ڈیٹا کے ساتھ بہترین کارکردگی کی ضمانت دیتا ہے۔
    d) یہ بنیادی طور پر جامد پیٹرن کی شناخت کے لیے استعمال ہوتا ہے۔
    **جواب: b**

2.  ہیومنائیڈ حرکت کے لیے RL میں، ایک بڑا **منفی انعام** عام طور پر اس کے لیے دیا جاتا ہے:
    a) آگے بڑھنا۔
    b) استحکام برقرار رکھنا۔
    c) گر جانا۔
    d) کم توانائی کی کھپت۔
    **جواب: c**

3.  **گہری تقویتی تعلیم (DRL)** ہیومنائیڈز کے لیے اہم ہے کیونکہ:
    a) یہ روبوٹ کی جسمانی ڈیزائن کو آسان بناتا ہے۔
    b) گہرے نیورل نیٹ ورکس اعلیٰ جہتی حالت اور عمل کی جگہوں کے لیے فنکشن تخمینہ فراہم کرتے ہیں۔
    c) یہ کسی بھی سینسر ڈیٹا کی ضرورت کو ختم کرتا ہے۔
    d) یہ صرف مجرد عمل کی جگہوں کے ساتھ کام کرتا ہے۔
    **جواب: b**

4.  PPO، SAC، اور TD3 جیسے الگورتھمز عام طور پر DRL میں ان کاموں کے لیے استعمال ہوتے ہیں:
    a) تصویر کی درجہ بندی۔
    b) قدرتی زبان کی تخلیق۔
    c) مسلسل کنٹرول (جیسے، جوڑوں کی پوزیشنیں، رفتار)۔
    d) صرف مجرد عمل کی جگہیں۔
    **جواب: c**

5.  روبوٹکس کے لیے RL میں **"sim-to-real gap"** سے مراد ہے:
    a) روبوٹ کو کمانڈز بھیجنے اور اس کی عملداری کے درمیان وقت کی تاخیر۔
    b) پالیسیوں کی کارکردگی میں فرق جب سمیولیشن سے جسمانی روبوٹ میں منتقل کیا جائے۔
    c) روبوٹ کی سمیولیٹڈ ظاہری شکل اور اس کی حقیقی دنیا کی شکل کے درمیان تفاوت۔
    d) حساس روبوٹس کی سمیولیٹ کرنے کے اخلاقی خدشات۔
    **جواب: b**

6.  **ڈومین بے ترتیبی** ایک تکنیک ہے جو sim-to-real gap کو اس طرح پر کرنے کے لیے استعمال ہوتی ہے:
    a) زیادہ درست سمیولیشن ماڈل بنانے کے لیے حقیقی دنیا کا ڈیٹا استعمال کرنا۔
    b) مضبوطی بڑھانے کے لیے تربیت کے دوران مختلف سمیولیشن پیرامیٹرز کو بے ترتیب کرنا۔
    c) جسمانی روبوٹ پر براہ راست پالیسیوں کو بہتر بنانا۔
    d) سمیولیٹڈ ماحول کی پیچیدگی کو آسان بنانا۔
    **جواب: b**

7.  **درجہ بندی تقویتی تعلیم (HRL)** کا بنیادی فائدہ کیا ہے؟
    a) یہ روبوٹ کے لیے درکار سینسرز کی تعداد کو کم کرتا ہے۔
    b) یہ پیچیدہ مسائل کو اعلیٰ سطح کے ذیلی کاموں اور نچلی سطح کی مہارتوں میں توڑ کر آسان بناتا ہے۔
    c) یہ انعامی فنکشنز کی ضرورت کو ختم کرتا ہے۔
    d) یہ یقینی بناتا ہے کہ پالیسیاں 100% بہترین ہیں۔
    **جواب: b**

8.  جب ایک RL کام صرف کامیاب تکمیل پر انعام فراہم کرتا ہے (جیسے، "چیز پکڑی گئی")، تو یہ اس سے متاثر ہوتا ہے:
    a) Sim-to-real gap۔
    b) کم انعامات۔
    c) تباہ کن بھولنا۔
    d) اعلیٰ جہتیت۔
    **جواب: b**

9.  جسمانی ہیومنائیڈز میں RL کے لیے **ایکسپلوریشن کی حفاظت** ایک اہم چیلنج کیوں ہے؟
    a) کیونکہ نئے اعمال تلاش کرنا غیر موثر حرکات کا باعث بن سکتا ہے۔
    b) کیونکہ بے ترتیب ایکسپلوریشن روبوٹ یا ماحول کو نقصان پہنچا سکتا ہے۔
    c) کیونکہ ایکسپلوریشن میں بہت زیادہ وقت لگتا ہے۔
    d) کیونکہ ایکسپلوریشن کے لیے بہت سارے سینسرز درکار ہیں۔
    **جواب: b**

10. پیچیدہ ہیومنائیڈ طرز عمل کے لیے مؤثر **انعامی فنکشنز** ڈیزائن کرنا چیلنجنگ ہے کیونکہ:
    a) انعامات کو ہمیشہ عددی طور پر متعین کرنا آسان ہوتا ہے۔
    b) ان کو ہاتھ سے تیار کرنا تاکہ وہ غیر ارادی ضمنی اثرات کے بغیر مطلوبہ طرز عمل کی طرف لے جائیں مشکل ہے۔
    c) روبوٹس ہمیشہ جانتے ہیں کہ ان کا مقصد کیا ہے۔
    d) انعامات صرف کام کے آغاز میں دیے جاتے ہیں۔
    **جواب: b**
