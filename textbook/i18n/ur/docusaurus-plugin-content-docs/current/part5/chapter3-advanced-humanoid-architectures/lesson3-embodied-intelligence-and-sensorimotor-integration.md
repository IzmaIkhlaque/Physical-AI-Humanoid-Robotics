---
sidebar_position: 3
sidebar_label: مجسم ذہانت اور حسی حرکی انضمام
---

# مجسم ذہانت اور حسی حرکی انضمام

## خلاصہ

*   **سبق 1 - کاگنیٹو آرکیٹیکچرز:** ذہین ہیومنائڈز کے لیے وسیع فریم ورکس۔
*   **سبق 2 - ہائبرڈ اور نیورو-سمبولک AI:** علامتی استدلال کو نیورل نیٹ ورکس کے ساتھ ملانا۔

روایتی AI اکثر ذہانت کو ایک غیر مجسم عمل کے طور پر دیکھتا ہے جو صرف کمپیوٹیشن میں رہتا ہے۔ تاہم، فزیکل ایجنٹس جیسے ہیومنائڈ روبوٹس کے لیے، ذہانت ان کے جسمانی ڈھانچے اور ماحول کے ساتھ اس کے تعاملات سے گہرا تعلق رکھتی ہے۔ یہی **مجسم ذہانت** کا جوہر ہے۔ مزید برآں، ایک روبوٹ کی سینسنگ صلاحیتوں (ادراک) اور اس کے موٹر اعمال (کنٹرول) کے درمیان ہموار ہم آہنگی – جسے **حسی حرکی انضمام** کہا جاتا ہے – مضبوط اور موافقت پذیر مجسم ذہانت کے حصول کے لیے بنیادی حیثیت رکھتی ہے۔

### 1. مجسم ذہانت: جسم کی اہمیت

*   **تعریف:** یہ خیال کہ ایک ذہین ایجنٹ کے ادراکی عمل اس کے جسمانی ڈھانچے اور دنیا کے ساتھ اس کے تعاملات سے تشکیل پاتے ہیں اور ان سے الگ نہیں ہو سکتے۔ ذہانت صرف تجریدی استدلال کے بارے میں نہیں ہے بلکہ ایک فزیکل ماحول میں عمل کرنے اور اسے سمجھنے کے بارے میں بھی ہے۔
*   **ہیومنائڈز کے لیے یہ کیوں اہم ہے:**
    *   **تصورات کی بنیاد:** تجریدی تصورات (مثلاً، "اوپر،" "بھاری،" "پکڑنا") جسمانی تعامل کے ذریعے معنی حاصل کرتے ہیں۔ ایک ہیومنائڈ کسی چیز کو اٹھاتے وقت لگنے والی قوت کو محسوس کرکے "بھاری" کا مطلب سیکھتا ہے۔
    *   **کمپیوٹیشن کو آسان بنانا:** جسم خود کچھ کمپیوٹیشن انجام دے سکتا ہے۔ مثال کے طور پر، ایک لچکدار ہاتھ قدرتی طور پر کسی چیز کی شکل کے مطابق ڈھل سکتا ہے، جس سے پکڑنے والے الگورتھم آسان ہو جاتے ہیں۔
    *   **براہ راست تعامل:** ہیومنائڈز براہ راست فزیکل دنیا میں کام کرتے ہیں، جس سے ان کا جسم ان پٹ (سینسنگ) اور آؤٹ پٹ (عمل) دونوں کے لیے بنیادی انٹرفیس بن جاتا ہے۔

### 2. حسی حرکی انضمام: ادراک اور عمل کو جوڑنا

حسی حرکی انضمام وہ متحرک عمل ہے جس کے ذریعے حسی معلومات کو موٹر اعمال کی رہنمائی اور ان کو بہتر بنانے کے لیے استعمال کیا جاتا ہے، اور اس کے برعکس، کس طرح موٹر اعمال فعال طور پر حسی ادراک کو تشکیل دیتے ہیں۔ یہ ایک مسلسل فیڈ بیک لوپ ہے۔

#### اہم پہلو:

*   **ادراک-عمل کا چکر:** حسی ان پٹ عمل کے فیصلوں کو مطلع کرتا ہے، اور نتیجے میں ہونے والا عمل ماحول کو بدل دیتا ہے، جس سے نیا حسی ان پٹ پیدا ہوتا ہے۔ یہ مسلسل چکر ذہین رویے کو چلاتا ہے۔
*   **حسی فیڈ بیک کے ذریعے موٹر کنٹرول کی رہنمائی:**
    *   **ویژول سروئنگ:** کیمرے کے فیڈ بیک کا استعمال کرتے ہوئے روبوٹ کے بازو کو ہدف تک درست طریقے سے رہنمائی کرنا۔
    *   **فورس کنٹرول:** چھونے والے اور فورس سینسرز کا استعمال کرتے ہوئے تعامل کی قوتوں کو منظم کرنا (مثلاً، پکڑنے یا دھکیلنے میں)۔
    *   **پروپریو سیپشن:** جسم کی پوزیشن اور جوڑوں کے زاویوں کا اندرونی احساس (انکودرز، IMUs سے) جو توازن اور حرکت کے لیے اہم فیڈ بیک فراہم کرتا ہے۔
*   **فعال ادراک:** موٹر اعمال کو ادراک کو *بہتر بنانے* کے لیے استعمال کیا جاتا ہے۔ مثال کے طور پر، ایک ہیومنائڈ آواز کو بہتر طریقے سے مقامی بنانے کے لیے اپنا سر موڑ سکتا ہے (موٹر عمل)، یا کسی چیز کو قریب سے دیکھنے کے لیے اس کے قریب جا سکتا ہے۔
*   **پریڈکٹیو کوڈنگ:** دماغ (یا روبوٹ کا کاگنیٹو سسٹم) اپنی موٹر کمانڈز کی بنیاد پر آنے والے حسی ڈیٹا کے بارے میں مسلسل پیشین گوئیاں پیدا کرتا ہے۔ پیشین گوئی اور اصل حسی ان پٹ (پیشین گوئی کی غلطی) کے درمیان کسی بھی تضاد کو اندرونی ماڈلز کو اپ ڈیٹ کرنے کے لیے استعمال کیا جاتا ہے۔

### 3. ڈیولپمنٹل روبوٹکس اور تعامل کے ذریعے سیکھنا

انسانی اور حیوانی نشوونما سے متاثر ہو کر، **ڈیولپمنٹل روبوٹکس** حسی حرکی تعامل کے ذریعے سیکھنے پر زور دیتا ہے، جو بنیادی اضطراری اعمال سے شروع ہو کر بتدریج مزید پیچیدہ مہارتیں حاصل کرتا ہے۔

*   **خود کی تلاش (ببلنگ):** ہیومنائڈز بے ترتیب حرکتیں کر سکتے ہیں اور اپنے جسم اور ماحول کے بارے میں جاننے کے لیے حسی نتائج کا مشاہدہ کر سکتے ہیں۔
*   **تجسس پر مبنی سیکھنا:** روبوٹس اندرونی طور پر نئے حالات کو تلاش کرنے اور نئی مہارتیں سیکھنے کے لیے متحرک ہوتے ہیں، یہاں تک کہ بیرونی انعامات کے بغیر بھی۔
*   **بوٹ اسٹریپنگ:** سادہ، فطری رویوں سے شروع کرنا اور انہیں مزید پیچیدہ رویوں کو سیکھنے کی بنیاد کے طور پر استعمال کرنا۔

**کوڈ اسنیپٹ مثال (آبجیکٹ کے تعامل کے لیے تصوراتی حسی حرکی لوپ):**

```python
import numpy as np
import time

class HumanoidObjectInteraction:
    def __init__(self):
        self.arm_position = np.array([0.0, 0.0, 0.0]) # Simplified 3D position
        self.target_object_pos = np.array([1.0, 0.5, 0.2]) # Where the object is
        self.gripper_force = 0.0
        self.visual_feedback = 0.0 # 0=no object, 1=object in view, >1=closer
        self.tactile_feedback = 0.0 # 0=no contact, >0=contact force
        self.internal_model = {"object_known_pos": None, "grasp_strength_needed": None}
        self.current_action = "idle"

    def sense(self):
        # Simulate visual feedback (is object in view, how close)
        distance_to_object = np.linalg.norm(self.target_object_pos - self.arm_position)
        if distance_to_object < 0.5:
            self.visual_feedback = 1.0 / distance_to_object # Closer, higher feedback
        else:
            self.visual_feedback = 0.0
        
        # Simulate tactile feedback (contact force when grasping)
        if self.current_action == "grasping":
            self.tactile_feedback = random.uniform(0.1, 10.0) # Random force
        else:
            self.tactile_feedback = 0.0

        # Update internal model based on perception
        if self.visual_feedback > 0.5 and self.internal_model["object_known_pos"] is None:
            self.internal_model["object_known_pos"] = self.target_object_pos
            print("Perception: Object detected and located.")

    def act(self):
        if self.current_action == "reaching":
            direction_to_object = (self.target_object_pos - self.arm_position)
            if np.linalg.norm(direction_to_object) > 0.1:
                self.arm_position += direction_to_object * 0.1 * random.uniform(0.8, 1.2) # Move towards object
            else:
                self.current_action = "grasping"
                print("Action: Reached object. Transitioning to grasping.")
        elif self.current_action == "grasping":
            if self.gripper_force < 5.0: # Keep increasing force
                self.gripper_force += 0.5
            else:
                print("Action: Grasping firm. Object held.")
                self.current_action = "held"
        elif self.current_action == "idle" and self.internal_model["object_known_pos"] is not None:
            self.current_action = "reaching"
            print("Action: Object known, initiating reaching.")
        
        # Adjust arm position based on tactile feedback during grasping (e.g., if too much force, retract slightly)
        if self.current_action == "grasping" and self.tactile_feedback > 8.0:
            print